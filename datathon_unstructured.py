# -*- coding: utf-8 -*-
"""datathon/unstructured.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CuiecQFYc4JeK1JST500IQPWE672HhBw
"""

from google.colab import drive
drive.mount('/content/drive')

import zipfile

with zipfile.ZipFile('/content/drive/My Drive/widsdatathon2025.zip', 'r') as zip_ref:
    zip_ref.extractall('/content/widsdatathon2025')

import numpy as np
import pandas as pd

# Standard library imports
import warnings

# Data manipulation and analysis libraries
import numpy as np
import pandas as pd

# Visualization libraries
import matplotlib.pyplot as plt
import seaborn as sns

# Machine learning libraries
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score
from lightgbm import LGBMClassifier

# Ignore warnings
warnings.simplefilter(action='ignore', category=Warning)

"""## Data loading

"""

import pandas as pd

try:
    df_categorical = pd.read_excel("/content/widsdatathon2025/TRAIN_NEW/TRAIN_CATEGORICAL_METADATA_new.xlsx")
    print(f"Shape of df_categorical: {df_categorical.shape}")

    df_connectome = pd.read_csv("/content/widsdatathon2025/TRAIN_NEW/TRAIN_FUNCTIONAL_CONNECTOME_MATRICES_new_36P_Pearson.csv", index_col=0)
    print(f"Shape of df_connectome: {df_connectome.shape}")

    df_quantitative = pd.read_excel("/content/widsdatathon2025/TRAIN_NEW/TRAIN_QUANTITATIVE_METADATA_new.xlsx")
    print(f"Shape of df_quantitative: {df_quantitative.shape}")

    df_solutions = pd.read_excel("/content/widsdatathon2025/TRAIN_NEW/TRAINING_SOLUTIONS.xlsx")
    print(f"Shape of df_solutions: {df_solutions.shape}")

except FileNotFoundError:
    print("One or more files not found.")

except Exception as e:
    print(f"An error occurred: {e}")

"""## Data exploration

"""

# Examine each DataFrame
print("df_categorical:")
display(df_categorical.info())
display(df_categorical.describe(include='all'))
for col in df_categorical.columns:
    print(f"\nColumn: {col}")
    print(f"Unique values: {df_categorical[col].unique()}")
    print(f"Missing values: {df_categorical[col].isnull().sum()}")

print("\ndf_connectome:")
display(df_connectome.info())
display(df_connectome.describe())
print(f"Missing values: {df_connectome.isnull().sum().sum()}")

print("\ndf_quantitative:")
display(df_quantitative.info())
display(df_quantitative.describe(include='all'))
for col in df_quantitative.columns:
    print(f"\nColumn: {col}")
    print(f"Unique values: {df_quantitative[col].unique()}")
    print(f"Missing values: {df_quantitative[col].isnull().sum()}")

print("\ndf_solutions:")
display(df_solutions.info())
display(df_solutions.describe(include='all'))
for col in df_solutions.columns:
    print(f"\nColumn: {col}")
    print(f"Unique values: {df_solutions[col].unique()}")
    print(f"Missing values: {df_solutions[col].isnull().sum()}")

# Analyze the solution variable
print("\nSolution Variable Analysis:")
print(f"Unique ADHD_Outcome values: {df_solutions['ADHD_Outcome'].unique()}")
print(f"Prediction task: Classification (ADHD_Outcome is likely a binary variable)")


# Identify potential key columns for merging
print("\nPotential Key Columns for Merging:")
print("The 'participant_id' column appears to be the common key across all DataFrames.")
print("However, verify if the same 'participant_id' exists in all four datasets.")

# Check for consistency in participant_ids across dataframes.
print(f"Number of unique participant_id in df_categorical:{len(df_categorical['participant_id'].unique())}")
print(f"Number of unique participant_id in df_connectome:{len(df_connectome.index.unique())}")
print(f"Number of unique participant_id in df_quantitative:{len(df_quantitative['participant_id'].unique())}")
print(f"Number of unique participant_id in df_solutions:{len(df_solutions['participant_id'].unique())}")

#Further analysis and documentation can be added if needed.

"""## Data preparation

"""

# Impute missing values
for col in df_categorical.columns:
    if df_categorical[col].dtype == 'object' or df_categorical[col].dtype.name == 'category':
        df_categorical[col] = df_categorical[col].fillna(df_categorical[col].mode()[0])
    else:
        df_categorical[col] = df_categorical[col].fillna(df_categorical[col].median())

for col in df_quantitative.columns:
    if df_quantitative[col].dtype == 'object' or df_quantitative[col].dtype.name == 'category':
        df_quantitative[col] = df_quantitative[col].fillna(df_quantitative[col].mode()[0])
    else:
        df_quantitative[col] = df_quantitative[col].fillna(df_quantitative[col].median())

# Convert data types if necessary
if not pd.api.types.is_numeric_dtype(df_categorical['Basic_Demos_Enroll_Year']):
    df_categorical['Basic_Demos_Enroll_Year'] = pd.to_numeric(df_categorical['Basic_Demos_Enroll_Year'], errors='coerce')
    df_categorical['Basic_Demos_Enroll_Year'] = df_categorical['Basic_Demos_Enroll_Year'].fillna(df_categorical['Basic_Demos_Enroll_Year'].median()).astype(int)

# Skip connectome matrix flattening
df_connectome_flattened = df_connectome.reset_index() # Just reset the index to align with other DataFrames

# Key verification and adjustment
# Ensure participant_id is consistent across all DataFrames
df_categorical['participant_id'] = df_categorical['participant_id'].astype(str)
df_quantitative['participant_id'] = df_quantitative['participant_id'].astype(str)
df_solutions['participant_id'] = df_solutions['participant_id'].astype(str)
df_connectome_flattened['participant_id'] = df_connectome_flattened['participant_id'].astype(str)

print(f"Shape of df_categorical: {df_categorical.shape}")
print(f"Shape of df_connectome_flattened: {df_connectome_flattened.shape}")
print(f"Shape of df_quantitative: {df_quantitative.shape}")
print(f"Shape of df_solutions: {df_solutions.shape}")

"""## Data merging

### Merge the four prepared DataFrames (`df_categorical`, `df_connectome_flattened`, `df_quantitative`, and `df_solutions`) into a single DataFrame called `df_merged`.

"""

# Merge the dataframes
df_temp1 = pd.merge(df_categorical, df_quantitative, on='participant_id', how='inner')
df_temp2 = pd.merge(df_temp1, df_connectome_flattened, on='participant_id', how='inner')
df_merged = pd.merge(df_temp2, df_solutions, on='participant_id', how='inner')

# Print the shape of the merged dataframe
print(f"Shape of df_merged: {df_merged.shape}")

# Display the first few rows of the merged dataframe
display(df_merged.head())

# Document any discrepancies
initial_rows = len(df_categorical)
rows_after_merge1 = len(df_temp1)
rows_after_merge2 = len(df_temp2)
rows_after_merge3 = len(df_merged)

print("\nRows lost during merging:")
print(f"Merge 1 (categorical & quantitative): {initial_rows - rows_after_merge1}")
print(f"Merge 2 (temp1 & connectome): {rows_after_merge1 - rows_after_merge2}")
print(f"Merge 3 (temp2 & solutions): {rows_after_merge2 - rows_after_merge3}")

# this section combining the four separate datasets (df_categorical, df_connectome_flattened, df_quantitative, and df_solutions) into a single dataset called df_merged.
# This is a crucial step because it allows us to analyze the relationships between different types of data for each participant.

"""## Feature engineering

### Re-engineer features, focusing on addressing the invalid connectome data.

"""

import pandas as pd
import numpy as np

def engineer_features(df):
    # 1. Connectivity Features
    connectome_cols = [col for col in df.columns if 'throw' in col]
    df['mean_connectivity'] = df[connectome_cols].mean(axis=1)
    # ... (other connectivity features) ...

    # 2. Interaction Terms
    df['connectivity_age_interaction'] = df['mean_connectivity'] * df['MRI_Track_Age_at_Scan']
    # ... (other interaction terms) ...

    # 3. Feature Transformation
    df['log_EHQ_EHQ_Total'] = np.log1p(df['EHQ_EHQ_Total'])

    # Impute missing values (consolidated)
    for col in df.select_dtypes(include=np.number).columns:
        df[col] = df[col].fillna(df[col].median())  # or other imputation method

    return df

df_merged = engineer_features(df_merged)

# Creating new features from existing ones to improve the performance of a machine learning model.

"""## Data splitting

### Split the merged dataset `df_merged` into training and testing sets.

"""

from sklearn.model_selection import train_test_split

# Define the target variable
target_variable = 'ADHD_Outcome'

# Separate features and target
X = df_merged.drop(target_variable, axis=1)
y = df_merged[target_variable]

# Convert participant_id to numeric
X['participant_id'] = pd.to_numeric(X['participant_id'], errors='coerce')
X['participant_id'] = X['participant_id'].fillna(X['participant_id'].median())

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Print shapes
print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)

# Print class proportions##
print("\ny_train class proportions:")
print(y_train.value_counts(normalize=True))
print("\ny_test class proportions:")
print(y_test.value_counts(normalize=True))

# Preparing the data for machine learning by
# splitting it into training and testing sets and performing a data type conversion.

"""## Model training

"""

from sklearn.ensemble import RandomForestClassifier

# Initialize the classifier
rf_model = RandomForestClassifier(random_state=42, n_jobs=-1)

# Train the model
rf_model.fit(X_train, y_train)

"""## Model

"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns

# Make predictions on the test set
y_pred = rf_model.predict(X_test)

# Calculate evaluation metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

try:
    auc_roc = roc_auc_score(y_test, y_pred)
    print(f"AUC-ROC: {auc_roc}")
except ValueError:
    print("AUC-ROC could not be calculated.")


print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1-score: {f1}")


# Generate confusion matrix
cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# Generate classification report
print(classification_report(y_test, y_pred))

# Document observations
print("\nObservations:")
print(f"The model achieved an accuracy of {accuracy:.2f}, precision of {precision:.2f}, recall of {recall:.2f}, and an F1-score of {f1:.2f}.")
if 'auc_roc' in locals():
    print(f"The AUC-ROC score is {auc_roc:.2f}.")
print("Consider the balance between precision, recall, and F1-score to determine the most appropriate metric for your use case.")

#acuuracy, the higher the better 0.68 means 68% of the restults were correctly predicted
# precision - higher the better, for us 69% precision
# recall / sensitivity - what proportion was correctly predcitied as positive (only focuss on positive predictions), for us 96%
# F1- Score, inteersection of precision and recall, measures quality and completeness of positive predictions
# our F1-score of 0.80 indicated a good balance between precision and recall

# confusion matrix shows counts of true poisitives, ture negatives, false positives, and false negatives

"""##EDA"""

import seaborn as sns
import matplotlib.pyplot as plt

sns.countplot(x='ADHD_Outcome', data=df_merged)
plt.title('Distribution of ADHD Outcome')
plt.show()

#visualizes the distribution of true yes's and true no's in the data set

import seaborn as sns
import matplotlib.pyplot as plt

# Select quantitative features
quantitative_features = df_quantitative.select_dtypes(include=['number']).columns

# Calculate correlation matrix
corr_matrix = df_merged[quantitative_features].corr()

# Plot heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix of Quantitative Features')
plt.show()

# Visually identifies relationships between the quantitative features in the dataset.
#By looking at the heatmap, you can easily see which features are strongly positively correlated
 #(red), strongly negatively correlated (blue), or have little to no correlation (lighter colors)

import seaborn as sns
import matplotlib.pyplot as plt

#Brain connectivity refers to the pattern of anatomical links (structural connectivity)
#and functional relationships (functional connectivity) between different regions of the brain.

connectivity_features = ['mean_connectivity', 'median_connectivity', 'std_connectivity']
for feature in connectivity_features:
    plt.figure(figsize=(8, 6))
    sns.histplot(df_merged[feature], kde=True)
    plt.title(f'Distribution of {feature}')
    plt.show()

#Structural connectivity describes the physical connections between brain areas,
#often through bundles of nerve fibers called white matter tracts.

#Functional connectivity describes the statistical dependencies between the
# activity of different brain regions, even if they are not directly physically connected.